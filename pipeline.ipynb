{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b504a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fec578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Directory 'models' already exists.\n",
      "Directory 'results' already exists.\n",
      "Data loaded successfully.\n",
      "Selected features: ['Shape_GG', 'AvgStress', 'cross', 'PathLength', 'pShape_KNN', 'pAvgStress', 'pCrossNo', 'pMinAng', 'pContinu', 'pGeode', '|V|', '|E|']\n",
      "Target column: accuracy\n",
      "Preprocessing completed using Pipeline.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "def create_directories(directories: List[str]):\n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Directory '{directory}' created.\")\n",
    "        else:\n",
    "            print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "MODEL_DIR = 'models'\n",
    "RESULT_DIR = 'results'\n",
    "\n",
    "create_directories([MODEL_DIR, RESULT_DIR])\n",
    "\n",
    "def load_and_preprocess_data(\n",
    "    data_path: str,\n",
    "    selected_features: List[str],\n",
    "    target_column: str\n",
    ") -> Tuple[np.ndarray, np.ndarray, Pipeline]:\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(data_path)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n",
    "    missing_features = set(selected_features + [target_column]) - set(data.columns)\n",
    "    if missing_features:\n",
    "        raise KeyError(f\"Missing columns in data: {missing_features}\")\n",
    "\n",
    "    features = data[selected_features]\n",
    "    target = data[target_column].values\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    print(f\"Target column: {target_column}\")\n",
    "\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    X_processed = preprocessing_pipeline.fit_transform(features)\n",
    "    print(\"Preprocessing completed using Pipeline.\")\n",
    "\n",
    "    return X_processed, target, preprocessing_pipeline\n",
    "\n",
    "DATA_PATH = 'data/fr_DID_mean.csv'\n",
    "\n",
    "SELECTED_FEATURES = [\n",
    "    'Shape_GG', 'AvgStress', 'cross', \n",
    "    'PathLength', 'pShape_KNN', 'pAvgStress', 'pCrossNo', \n",
    "    'pMinAng', 'pContinu', 'pGeode', \n",
    "    '|V|', '|E|'\n",
    "]\n",
    "\n",
    "TARGET_COLUMN = 'accuracy'\n",
    "\n",
    "# Load and preprocess data\n",
    "X, y, preprocessing_pipeline = load_and_preprocess_data(DATA_PATH, SELECTED_FEATURES, TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feff02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int = 1):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "def evaluate_and_plot(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    model_name: str\n",
    "):\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} - MSE: {mse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.7)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(f'{model_name}: Actual vs Predicted')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f'{model_name}_actual_vs_predicted_{timestamp}.png'\n",
    "    plt.savefig(os.path.join(RESULT_DIR, plot_filename))\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {plot_filename}\")\n",
    "    \n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'{model_name}: Residuals vs Predicted')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f'{model_name}_residuals_{timestamp}.png'\n",
    "    plt.savefig(os.path.join(RESULT_DIR, plot_filename))\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {plot_filename}\")\n",
    "\n",
    "def train_nn_model(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    epochs: int = 200,\n",
    "    batch_size: int = 64,\n",
    "    learning_rate: float = 0.001,\n",
    "    patience: int = 10\n",
    "):\n",
    "\n",
    "    train_dataset = GraphDataset(X_train, y_train)\n",
    "    val_dataset = GraphDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SimpleNN(input_size=X_train.shape[1]).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                     factor=0.5, patience=5, \n",
    "                                                     verbose=True)\n",
    "    \n",
    "    best_model_state = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(\"Validation loss improved. Best model updated.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement in validation loss for {epochs_no_improve} epoch(s).\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    model_filename = f'best_nn_model_{timestamp}.pth'\n",
    "    torch.save(model.state_dict(), os.path.join(MODEL_DIR, model_filename))\n",
    "    print(f\"Best model saved to {os.path.join(MODEL_DIR, model_filename)}.\")\n",
    "\n",
    "    model.eval()\n",
    "    test_dataset = GraphDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            y_pred.append(outputs.cpu().numpy())\n",
    "            y_true.append(labels.numpy())\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    evaluate_and_plot(y_true, y_pred, \"Neural_Network\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_gradient_boosting(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    param_distributions: Optional[Dict] = None,\n",
    "    n_iter: int = 100\n",
    ") -> GradientBoostingRegressor:\n",
    "\n",
    "    if param_distributions:\n",
    "        print(\"Starting Randomized Search for Gradient Boosting...\")\n",
    "        gbr = GradientBoostingRegressor(random_state=42)\n",
    "        randomized_search = RandomizedSearchCV(\n",
    "            estimator=gbr,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            verbose=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        randomized_search.fit(X_train, y_train)\n",
    "        best_model = randomized_search.best_estimator_\n",
    "        print(f\"Best Gradient Boosting Params: {randomized_search.best_params_}\")\n",
    "    else:\n",
    "        best_model = GradientBoostingRegressor(random_state=42)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        print(\"Gradient Boosting model trained with default parameters.\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def run_experiment(\n",
    "    X: np.ndarray,\n",
    "    target: np.ndarray,\n",
    "    model_type: str = 'nn',\n",
    "    param_distributions: Optional[Dict] = None,\n",
    "    n_iter: int = 100\n",
    "):\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, target, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, random_state=42  # 0.25 x 0.8 = 0.2\n",
    "    )\n",
    "    \n",
    "    if model_type == 'nn':\n",
    "        print(f\"\\nRunning Neural Network with features: {SELECTED_FEATURES}\")\n",
    "        model = train_nn_model(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    elif model_type == 'boosting':\n",
    "        print(f\"\\nRunning Gradient Boosting with features: {SELECTED_FEATURES}\")\n",
    "        best_model = train_gradient_boosting(X_train, y_train, param_distributions, n_iter)\n",
    "        # Evaluation on test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        evaluate_and_plot(y_test, y_pred, \"Gradient_Boosting\")\n",
    "        # Save the trained model\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = os.path.join(MODEL_DIR, f'best_gradient_boosting_model_{timestamp}.pkl')\n",
    "        joblib.dump(best_model, model_path)\n",
    "        print(f\"Gradient Boosting model saved to {model_path}.\")\n",
    "    else:\n",
    "        print(f\"Unsupported model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4bb8f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiments...\n",
      "\n",
      "Experiment: Predicting Accuracy - Neural Network\n",
      "\n",
      "Running Neural Network with features: ['Shape_GG', 'AvgStress', 'cross', 'PathLength', 'pShape_KNN', 'pAvgStress', 'pCrossNo', 'pMinAng', 'pContinu', 'pGeode', '|V|', '|E|']\n",
      "Epoch [1/200], Train Loss: 0.6918, Val Loss: 0.7057\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [2/200], Train Loss: 0.6639, Val Loss: 0.6973\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [3/200], Train Loss: 0.6560, Val Loss: 0.6998\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch [4/200], Train Loss: 0.6516, Val Loss: 0.7030\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch [5/200], Train Loss: 0.6479, Val Loss: 0.6893\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [6/200], Train Loss: 0.6417, Val Loss: 0.6934\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch [7/200], Train Loss: 0.6404, Val Loss: 0.7033\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch [8/200], Train Loss: 0.6391, Val Loss: 0.6990\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch [9/200], Train Loss: 0.6341, Val Loss: 0.6964\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch [10/200], Train Loss: 0.6285, Val Loss: 0.6927\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch [11/200], Train Loss: 0.6283, Val Loss: 0.6856\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [12/200], Train Loss: 0.6242, Val Loss: 0.6864\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch [13/200], Train Loss: 0.6175, Val Loss: 0.7013\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch [14/200], Train Loss: 0.6172, Val Loss: 0.7045\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch [15/200], Train Loss: 0.6107, Val Loss: 0.6919\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch [16/200], Train Loss: 0.6010, Val Loss: 0.7141\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch [17/200], Train Loss: 0.5975, Val Loss: 0.7014\n",
      "Epoch 00017: reducing learning rate of group 0 to 5.0000e-04.\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch [18/200], Train Loss: 0.5831, Val Loss: 0.7080\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch [19/200], Train Loss: 0.5778, Val Loss: 0.7039\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch [20/200], Train Loss: 0.5714, Val Loss: 0.7151\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch [21/200], Train Loss: 0.5716, Val Loss: 0.7085\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "Best model saved to models\\best_nn_model_20241023_111552.pth.\n",
      "Neural_Network - MSE: 0.6608, MAE: 0.6509, RÂ²: 0.0950\n",
      "Saved plot: Neural_Network_actual_vs_predicted_20241023_111554.png\n",
      "Saved plot: Neural_Network_residuals_20241023_111554.png\n",
      "\n",
      "Experiment: Predicting Accuracy - Gradient Boosting\n",
      "\n",
      "Running Gradient Boosting with features: ['Shape_GG', 'AvgStress', 'cross', 'PathLength', 'pShape_KNN', 'pAvgStress', 'pCrossNo', 'pMinAng', 'pContinu', 'pGeode', '|V|', '|E|']\n",
      "Starting Randomized Search for Gradient Boosting...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "130 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\69418\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [-0.74567222 -0.8287933  -0.79849472         nan -0.72888008 -0.73487471\n",
      " -0.78404358 -0.68279095 -0.76127948         nan -0.8123811  -0.78571473\n",
      " -0.81847232 -0.68197049 -0.85934748 -0.7660828  -0.69894654 -0.66970843\n",
      " -0.76410043         nan         nan -0.81686834 -0.76418425         nan\n",
      " -0.82619356 -0.82376934         nan         nan -0.82263578 -0.83762453\n",
      " -0.79249856 -0.79643144 -0.68589889 -0.6728007  -0.81855266 -0.72574599\n",
      " -0.83115742 -0.75044647 -0.80620468         nan         nan -0.78721327\n",
      " -0.79449402         nan         nan -0.8516737          nan         nan\n",
      " -0.801726           nan -0.6708259          nan -0.74596135 -0.8401575\n",
      "         nan         nan         nan -0.77826709         nan         nan\n",
      " -0.89470981 -0.73588837         nan -0.81702679 -0.79180026 -0.7943873\n",
      " -0.90903946         nan -0.86033896 -0.8386298  -0.76069944 -0.68628453\n",
      " -0.76909953 -0.70841509 -0.79556022         nan -0.8072112  -0.74876454\n",
      "         nan -0.74420098 -0.8578016          nan -0.81579697 -0.71941998\n",
      " -0.8498674          nan -0.71621218 -0.68547321 -0.88027636 -0.83118309\n",
      " -0.66122139 -0.86086853 -0.73166639 -0.76468822 -0.78743467 -0.82279249\n",
      " -0.72993958 -0.79622472 -0.73889408 -0.80198464]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Gradient Boosting Params: {'subsample': 0.8, 'n_estimators': 500, 'min_samples_split': 16, 'min_samples_leaf': 17, 'max_features': None, 'max_depth': 3, 'learning_rate': 0.01}\n",
      "Gradient_Boosting - MSE: 0.6527, MAE: 0.6646, RÂ²: 0.1061\n",
      "Saved plot: Gradient_Boosting_actual_vs_predicted_20241023_111655.png\n",
      "Saved plot: Gradient_Boosting_residuals_20241023_111655.png\n",
      "Gradient Boosting model saved to models\\best_gradient_boosting_model_20241023_111655.pkl.\n",
      "\n",
      "Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting experiments...\")\n",
    "\n",
    "    PARAM_DISTRIBUTIONS = {\n",
    "        'n_estimators': np.arange(100, 1001, 100),\n",
    "        'learning_rate': np.linspace(0.01, 0.3, 30),\n",
    "        'max_depth': np.arange(3, 11, 1),\n",
    "        'min_samples_split': np.arange(2, 21, 2),\n",
    "        'min_samples_leaf': np.arange(1, 21, 2),\n",
    "        'subsample': np.linspace(0.5, 1.0, 6),\n",
    "        'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "    }\n",
    "\n",
    "    print(\"\\nExperiment: Predicting Accuracy - Neural Network\")\n",
    "    run_experiment(X, y, model_type='nn')\n",
    "\n",
    "    print(\"\\nExperiment: Predicting Accuracy - Gradient Boosting\")\n",
    "    run_experiment(X, y, model_type='boosting', param_distributions=PARAM_DISTRIBUTIONS, n_iter=100)\n",
    "\n",
    "    print(\"\\nExperiments completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
