{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67eae67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import xgboost as xgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6538f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Directory 'models' already exists.\n",
      "Directory 'results' already exists.\n",
      "Data loaded successfully.\n",
      "Selected features: ['Shape_GG', 'AvgStress', 'cross', 'PathLength', 'pShape_KNN', 'pAvgStress', 'pCrossNo', 'pMinAng', 'pContinu', 'pGeode', '|V|', '|E|', 'Shape_RNG', 'Shape_EMST', 'Shape_AlphaShape', 'Shape_KNN', 'RegularStress', 'AvgScaledStress', 'angleM', 'angleD', 'minAng', 'edgeM', 'edgeD', 'minVertx', 'finVertx', 'minPtEdg', 'pShape_RNG', 'pShape_EMST', 'pRegularStress', 'pAvgScaledStress', 'pAngMean', 'pContinu', 'pGeode', 'YN', 'Effort', 'Time']\n",
      "Target column: accuracy\n",
      "Loaded data with 5542 samples and 36 features.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "def create_directories(directories: List[str]):\n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Directory '{directory}' created.\")\n",
    "        else:\n",
    "            print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "MODEL_DIR = 'models'\n",
    "RESULT_DIR = 'results'\n",
    "\n",
    "create_directories([MODEL_DIR, RESULT_DIR])\n",
    "\n",
    "def load_data(data_path: str,selected_features: Optional[List[str]] = None,target_column: str = 'accuracy'):\n",
    "    data = pd.read_csv(data_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    if selected_features is None:\n",
    "        selected_features = data.columns.drop(target_column).tolist()\n",
    "        print(\"Using all available features except the target.\")\n",
    "\n",
    "    missing_features = set(selected_features + [target_column]) - set(data.columns)\n",
    "    if missing_features:\n",
    "        raise KeyError(f\"Missing columns in data: {missing_features}\")\n",
    "\n",
    "    # Transformation target variable transformation if skewed\n",
    "    if data[target_column].skew() > 1:\n",
    "        print(\"Target variable is skewed. Applying log transformation.\")\n",
    "        data[target_column] = np.log1p(data[target_column])\n",
    "\n",
    "    features = data[selected_features]\n",
    "    target = data[target_column].values\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    print(f\"Target column: {target_column}\")\n",
    "\n",
    "    return features, target\n",
    "\n",
    "DATA_PATH = 'data/fr_DID_mean.csv'\n",
    "\n",
    "# Feature set\n",
    "SELECTED_FEATURES = [\n",
    "    'Shape_GG', 'AvgStress', 'cross', \n",
    "    'PathLength', 'pShape_KNN', 'pAvgStress', 'pCrossNo', \n",
    "    'pMinAng', 'pContinu', 'pGeode', \n",
    "    '|V|', '|E|',\n",
    "    'Shape_RNG', 'Shape_EMST', 'Shape_AlphaShape', 'Shape_KNN',\n",
    "    'RegularStress', 'AvgScaledStress', 'angleM', 'angleD', 'minAng',\n",
    "    'edgeM', 'edgeD', 'minVertx', 'finVertx', 'minPtEdg',\n",
    "    'pShape_RNG', 'pShape_EMST', 'pRegularStress', 'pAvgScaledStress',\n",
    "    'pAngMean', 'pContinu', 'pGeode', 'YN', 'Effort', 'Time'\n",
    "]\n",
    "\n",
    "TARGET_COLUMN = 'accuracy'\n",
    "\n",
    "features, target = load_data(DATA_PATH, selected_features=SELECTED_FEATURES, target_column=TARGET_COLUMN)\n",
    "\n",
    "print(f\"Loaded data with {features.shape[0]} samples and {features.shape[1]} features.\")\n",
    "\n",
    "def create_preprocessing_pipeline(selected_features: List[str], k_best: int = 30) -> Pipeline:\n",
    "    # Preprocessing pipeline with imputation, polynomial features, scaling, and feature selection.\n",
    "    preprocessing_steps = [\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_regression, k=k_best))\n",
    "    ]\n",
    "\n",
    "    preprocessing_pipeline = Pipeline(preprocessing_steps)\n",
    "    return preprocessing_pipeline\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Neural Network architecture\n",
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int = 1):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "# Evaluation and plotting\n",
    "def evaluate_and_plot(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    model_name: str,\n",
    "    transform_back: bool = False\n",
    "):\n",
    "\n",
    "    if transform_back:\n",
    "        y_true = np.expm1(y_true)\n",
    "        y_pred = np.expm1(y_pred)\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} - MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Plot actual vs Predicted\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, alpha=0.6)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(f'{model_name}: Actual vs Predicted')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f'{model_name}_actual_vs_predicted_{timestamp}.png'\n",
    "    plt.savefig(os.path.join(RESULT_DIR, plot_filename))\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {plot_filename}\")\n",
    "    \n",
    "    # Plot residuals vs Predicted\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, alpha=0.6)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'{model_name}: Residuals vs Predicted')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f'{model_name}_residuals_{timestamp}.png'\n",
    "    plt.savefig(os.path.join(RESULT_DIR, plot_filename))\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {plot_filename}\")\n",
    "\n",
    "    # Feature Importance for tree based models\n",
    "    if model_name in ['Gradient_Boosting', 'XGBoost']:\n",
    "        try:\n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importances = best_model.feature_importances_\n",
    "                feature_names = preprocessing_pipeline.named_steps['feature_selection'].get_support(indices=True)\n",
    "                selected_feature_names = [SELECTED_FEATURES[i] for i in feature_names]\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.barplot(x=importances[indices][:20], y=np.array(selected_feature_names)[indices][:20])\n",
    "                plt.title('Top Feature Importances')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"Model does not have feature_importances_ attribute.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting feature importances: {e}\")\n",
    "\n",
    "# Train Neural Network\n",
    "def train_nn_model(\n",
    "    preprocessing_pipeline: Pipeline,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    epochs: int = 300,\n",
    "    batch_size: int = 128,\n",
    "    learning_rate: float = 0.001,\n",
    "    patience: int = 20\n",
    "):\n",
    "    \n",
    "    X_train_processed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "    X_val_processed = preprocessing_pipeline.transform(X_val)\n",
    "    X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "    \n",
    "    print(f\"Training data processed shape: {X_train_processed.shape}\")\n",
    "    print(f\"Validation data processed shape: {X_val_processed.shape}\")\n",
    "    print(f\"Test data processed shape: {X_test_processed.shape}\")\n",
    "    \n",
    "    train_dataset = GraphDataset(X_train_processed, y_train)\n",
    "    val_dataset = GraphDataset(X_val_processed, y_val)\n",
    "    test_dataset = GraphDataset(X_test_processed, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = X_train_processed.shape[1]\n",
    "    model = ImprovedNN(input_size=input_size).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                     factor=0.5, patience=5, \n",
    "                                                     verbose=True)\n",
    "    \n",
    "    best_model_state = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(\"Validation loss improved. Best model updated.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement in validation loss for {epochs_no_improve} epoch(s).\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    model_filename = f'best_nn_model_{timestamp}.pth'\n",
    "    torch.save(model.state_dict(), os.path.join(MODEL_DIR, model_filename))\n",
    "    print(f\"Best Neural Network model saved to {os.path.join(MODEL_DIR, model_filename)}.\")\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            y_pred.append(outputs.cpu().numpy())\n",
    "            y_true.append(labels.numpy())\n",
    "    y_pred = np.concatenate(y_pred, axis=0).flatten()\n",
    "    y_true = np.concatenate(y_true, axis=0).flatten()\n",
    "\n",
    "    evaluate_and_plot(y_true, y_pred, \"Neural_Network\", transform_back=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train Gradient Boosting\n",
    "def train_gradient_boosting(X_train: np.ndarray,y_train: np.ndarray,param_distributions: Optional[Dict] = None,n_iter: int = 50,use_xgboost: bool = True):\n",
    "\n",
    "    if use_xgboost:\n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "    else:\n",
    "        model = GradientBoostingRegressor(random_state=42)\n",
    "    \n",
    "    if param_distributions:\n",
    "        print(\"Starting Randomized Search for Gradient Boosting...\")\n",
    "        randomized_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=5,\n",
    "            scoring='r2',  \n",
    "            verbose=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        randomized_search.fit(X_train, y_train)\n",
    "        best_model = randomized_search.best_estimator_\n",
    "        print(f\"Best Gradient Boosting Params: {randomized_search.best_params_}\")\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        best_model = model\n",
    "        print(\"Gradient Boosting model trained with default parameters.\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Experiment runner\n",
    "def run_experiment(\n",
    "    features: pd.DataFrame,\n",
    "    target: np.ndarray,\n",
    "    model_type: str = 'nn',\n",
    "    param_distributions: Optional[Dict] = None,\n",
    "    n_iter: int = 50\n",
    "):\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        features, target, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, random_state=42  # 0.25 x 0.8 = 0.2\n",
    "    )\n",
    "    \n",
    "    if model_type == 'nn':\n",
    "        print(f\"\\nRunning Neural Network with features: {SELECTED_FEATURES}\")\n",
    "        preprocessing_pipeline = create_preprocessing_pipeline(SELECTED_FEATURES, k_best=30)\n",
    "        model = train_nn_model(\n",
    "            preprocessing_pipeline,\n",
    "            X_train.values, y_train,\n",
    "            X_val.values, y_val,\n",
    "            X_test.values, y_test\n",
    "        )\n",
    "    elif model_type in ['boosting', 'xgboost']:\n",
    "        print(f\"\\nRunning {'XGBoost' if model_type == 'xgboost' else 'Gradient Boosting'} with features: {SELECTED_FEATURES}\")\n",
    "        preprocessing_pipeline = create_preprocessing_pipeline(SELECTED_FEATURES, k_best=30)\n",
    "        X_train_processed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "        X_val_processed = preprocessing_pipeline.transform(X_val)\n",
    "        X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "        best_model = train_gradient_boosting(\n",
    "            X_train_processed,\n",
    "            y_train,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            use_xgboost=(model_type == 'xgboost')\n",
    "        )\n",
    "\n",
    "        y_pred = best_model.predict(X_test_processed)\n",
    "        evaluate_and_plot(y_test, y_pred, \"XGBoost\" if model_type == 'xgboost' else \"Gradient_Boosting\", transform_back=True)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_filename = f'best_xgboost_model_{timestamp}.pkl' if model_type == 'xgboost' else f'best_gradient_boosting_model_{timestamp}.pkl'\n",
    "        joblib.dump(best_model, os.path.join(MODEL_DIR, model_filename))\n",
    "        print(f\"{'XGBoost' if model_type == 'xgboost' else 'Gradient Boosting'} model saved to {os.path.join(MODEL_DIR, model_filename)}.\")\n",
    "    else:\n",
    "        print(f\"Unsupported model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e48cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiments...\n",
      "\n",
      "Experiment: Predicting Accuracy - Neural Network\n",
      "\n",
      "Running Neural Network with features: ['Shape_GG', 'AvgStress', 'cross', 'PathLength', 'pShape_KNN', 'pAvgStress', 'pCrossNo', 'pMinAng', 'pContinu', 'pGeode', '|V|', '|E|', 'Shape_RNG', 'Shape_EMST', 'Shape_AlphaShape', 'Shape_KNN', 'RegularStress', 'AvgScaledStress', 'angleM', 'angleD', 'minAng', 'edgeM', 'edgeD', 'minVertx', 'finVertx', 'minPtEdg', 'pShape_RNG', 'pShape_EMST', 'pRegularStress', 'pAvgScaledStress', 'pAngMean', 'pContinu', 'pGeode', 'YN', 'Effort', 'Time']\n",
      "Training data processed shape: (3324, 30)\n",
      "Validation data processed shape: (1109, 30)\n",
      "Test data processed shape: (1109, 30)\n",
      "Epoch [1/300], Train Loss: 0.193560, Val Loss: 0.023254\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [2/300], Train Loss: 0.049258, Val Loss: 0.020495\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [3/300], Train Loss: 0.044559, Val Loss: 0.015216\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [4/300], Train Loss: 0.041332, Val Loss: 0.024404\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch [5/300], Train Loss: 0.036898, Val Loss: 0.016858\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch [6/300], Train Loss: 0.033590, Val Loss: 0.014739\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [7/300], Train Loss: 0.038192, Val Loss: 0.019785\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch [8/300], Train Loss: 0.032774, Val Loss: 0.018218\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch [9/300], Train Loss: 0.035187, Val Loss: 0.016634\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch [10/300], Train Loss: 0.027297, Val Loss: 0.014463\n",
      "Validation loss improved. Best model updated.\n",
      "Epoch [11/300], Train Loss: 0.030467, Val Loss: 0.016040\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch [12/300], Train Loss: 0.033479, Val Loss: 0.023664\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch [13/300], Train Loss: 0.033339, Val Loss: 0.021914\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch [14/300], Train Loss: 0.028205, Val Loss: 0.021402\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch [15/300], Train Loss: 0.031147, Val Loss: 0.025890\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch [16/300], Train Loss: 0.027519, Val Loss: 0.020348\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch [17/300], Train Loss: 0.028901, Val Loss: 0.019077\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch [18/300], Train Loss: 0.030558, Val Loss: 0.019490\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch [19/300], Train Loss: 0.027063, Val Loss: 0.018638\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch [20/300], Train Loss: 0.030007, Val Loss: 0.016775\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Epoch [21/300], Train Loss: 0.028085, Val Loss: 0.016469\n",
      "No improvement in validation loss for 11 epoch(s).\n",
      "Epoch [22/300], Train Loss: 0.026504, Val Loss: 0.018390\n",
      "Epoch 00022: reducing learning rate of group 0 to 2.5000e-04.\n",
      "No improvement in validation loss for 12 epoch(s).\n",
      "Epoch [23/300], Train Loss: 0.025790, Val Loss: 0.018739\n",
      "No improvement in validation loss for 13 epoch(s).\n",
      "Epoch [24/300], Train Loss: 0.030757, Val Loss: 0.023232\n",
      "No improvement in validation loss for 14 epoch(s).\n",
      "Epoch [25/300], Train Loss: 0.026685, Val Loss: 0.020139\n",
      "No improvement in validation loss for 15 epoch(s).\n",
      "Epoch [26/300], Train Loss: 0.029126, Val Loss: 0.016603\n",
      "No improvement in validation loss for 16 epoch(s).\n",
      "Epoch [27/300], Train Loss: 0.027728, Val Loss: 0.018243\n",
      "No improvement in validation loss for 17 epoch(s).\n",
      "Epoch [28/300], Train Loss: 0.027395, Val Loss: 0.018603\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.2500e-04.\n",
      "No improvement in validation loss for 18 epoch(s).\n",
      "Epoch [29/300], Train Loss: 0.025082, Val Loss: 0.020223\n",
      "No improvement in validation loss for 19 epoch(s).\n",
      "Epoch [30/300], Train Loss: 0.027098, Val Loss: 0.020812\n",
      "No improvement in validation loss for 20 epoch(s).\n",
      "Early stopping triggered.\n",
      "Best Neural Network model saved to models\\best_nn_model_20241113_092715.pth.\n",
      "Neural_Network - MSE: 0.0120, MAE: 0.0715, R²: 0.9746\n",
      "Saved plot: Neural_Network_actual_vs_predicted_20241113_092717.png\n",
      "Saved plot: Neural_Network_residuals_20241113_092717.png\n",
      "\n",
      "Experiment: Predicting Accuracy - XGBoost\n",
      "\n",
      "Running XGBoost with features: ['Shape_GG', 'AvgStress', 'cross', 'PathLength', 'pShape_KNN', 'pAvgStress', 'pCrossNo', 'pMinAng', 'pContinu', 'pGeode', '|V|', '|E|', 'Shape_RNG', 'Shape_EMST', 'Shape_AlphaShape', 'Shape_KNN', 'RegularStress', 'AvgScaledStress', 'angleM', 'angleD', 'minAng', 'edgeM', 'edgeD', 'minVertx', 'finVertx', 'minPtEdg', 'pShape_RNG', 'pShape_EMST', 'pRegularStress', 'pAvgScaledStress', 'pAngMean', 'pContinu', 'pGeode', 'YN', 'Effort', 'Time']\n",
      "Starting Randomized Search for Gradient Boosting...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Gradient Boosting Params: {'subsample': 0.7, 'n_estimators': 400, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.03, 'gamma': 0, 'colsample_bytree': 0.9}\n",
      "XGBoost - MSE: 0.0066, MAE: 0.0483, R²: 0.9861\n",
      "Saved plot: XGBoost_actual_vs_predicted_20241113_092725.png\n",
      "Saved plot: XGBoost_residuals_20241113_092725.png\n",
      "Error plotting feature importances: name 'best_model' is not defined\n",
      "XGBoost model saved to models\\best_xgboost_model_20241113_092726.pkl.\n",
      "\n",
      "Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting experiments...\")\n",
    "\n",
    "    # Hyperparameters for XGBoost\n",
    "    PARAM_DISTRIBUTIONS_XGB = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'learning_rate': np.linspace(0.01, 0.3, 30),\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'min_child_weight': [1, 2, 3, 4, 5],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3, 0.4]\n",
    "    }\n",
    "\n",
    "    # Neural Network\n",
    "    print(\"\\nExperiment: Predicting Accuracy - Neural Network\")\n",
    "    run_experiment(features, target, model_type='nn')\n",
    "\n",
    "    # XGBoost\n",
    "    print(\"\\nExperiment: Predicting Accuracy - XGBoost\")\n",
    "    run_experiment(features, target, model_type='xgboost', param_distributions=PARAM_DISTRIBUTIONS_XGB, n_iter=50)\n",
    "\n",
    "    print(\"\\nExperiments completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
